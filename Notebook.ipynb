{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader  # For custom dataset and batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ./data/train-images-idx3-ubyte.gz...\n",
      "Downloaded ./data/train-images-idx3-ubyte.gz\n",
      "Downloading ./data/train-labels-idx1-ubyte.gz...\n",
      "Downloaded ./data/train-labels-idx1-ubyte.gz\n",
      "Downloading ./data/t10k-images-idx3-ubyte.gz...\n",
      "Downloaded ./data/t10k-images-idx3-ubyte.gz\n",
      "Downloading ./data/t10k-labels-idx1-ubyte.gz...\n",
      "Downloaded ./data/t10k-labels-idx1-ubyte.gz\n",
      "Extracting ./data/t10k-images-idx3-ubyte.gz...\n",
      "Extracted to ./data/t10k-images-idx3-ubyte\n",
      "Extracting ./data/train-images-idx3-ubyte.gz...\n",
      "Extracted to ./data/train-images-idx3-ubyte\n",
      "Extracting ./data/train-labels-idx1-ubyte.gz...\n",
      "Extracted to ./data/train-labels-idx1-ubyte\n",
      "Extracting ./data/t10k-labels-idx1-ubyte.gz...\n",
      "Extracted to ./data/t10k-labels-idx1-ubyte\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import gzip\n",
    "import shutil\n",
    "\n",
    "# Updated URLs for MNIST dataset\n",
    "urls = [\n",
    "    \"https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\",\n",
    "    \"https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\",\n",
    "    \"https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\",\n",
    "    \"https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\"\n",
    "]\n",
    "\n",
    "# Directory to store data\n",
    "os.makedirs(\"./data\", exist_ok=True)\n",
    "\n",
    "# Function to download files\n",
    "def download_file(url, save_dir=\"./data\"):\n",
    "    filename = os.path.join(save_dir, url.split(\"/\")[-1])\n",
    "    response = requests.get(url, stream=True)\n",
    "    if response.status_code == 200:\n",
    "        print(f\"Downloading {filename}...\")\n",
    "        with open(filename, \"wb\") as f:\n",
    "            for chunk in response.iter_content(chunk_size=1024):\n",
    "                f.write(chunk)\n",
    "        print(f\"Downloaded {filename}\")\n",
    "    else:\n",
    "        print(f\"Failed to download {url}: {response.status_code}\")\n",
    "\n",
    "# Download all files\n",
    "for url in urls:\n",
    "    download_file(url)\n",
    "\n",
    "# Function to extract .gz files\n",
    "def extract_gz_files(source_dir=\"./data\"):\n",
    "    for file in os.listdir(source_dir):\n",
    "        if file.endswith(\".gz\"):\n",
    "            file_path = os.path.join(source_dir, file)\n",
    "            extracted_path = file_path.replace(\".gz\", \"\")\n",
    "            print(f\"Extracting {file_path}...\")\n",
    "            with gzip.open(file_path, 'rb') as f_in:\n",
    "                with open(extracted_path, 'wb') as f_out:\n",
    "                    shutil.copyfileobj(f_in, f_out)\n",
    "            print(f\"Extracted to {extracted_path}\")\n",
    "\n",
    "# Extract all downloaded files\n",
    "extract_gz_files(\"./data\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Images Shape: (60000, 28, 28)\n",
      "Train Labels Shape: (60000,)\n",
      "Test Images Shape: (10000, 28, 28)\n",
      "Test Labels Shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import struct\n",
    "\n",
    "def load_images(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        # Read the header\n",
    "        magic, num_images, rows, cols = struct.unpack(\">IIII\", f.read(16))\n",
    "        assert magic == 2051, \"Invalid magic number for image file!\"\n",
    "        # Read the image data and reshape it\n",
    "        images = np.frombuffer(f.read(), dtype=np.uint8).reshape(num_images, rows, cols)\n",
    "    return images\n",
    "\n",
    "def load_labels(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        # Read the header\n",
    "        magic, num_labels = struct.unpack(\">II\", f.read(8))\n",
    "        assert magic == 2049, \"Invalid magic number for label file!\"\n",
    "        # Read the label data\n",
    "        labels = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "    return labels\n",
    "\n",
    "# Load training data\n",
    "train_images = load_images('./data/train-images-idx3-ubyte')\n",
    "train_labels = load_labels('./data/train-labels-idx1-ubyte')\n",
    "\n",
    "# Load test data\n",
    "test_images = load_images('./data/t10k-images-idx3-ubyte')\n",
    "test_labels = load_labels('./data/t10k-labels-idx1-ubyte')\n",
    "\n",
    "# Print shapes for verification\n",
    "print(f\"Train Images Shape: {train_images.shape}\")\n",
    "print(f\"Train Labels Shape: {train_labels.shape}\")\n",
    "print(f\"Test Images Shape: {test_images.shape}\")\n",
    "print(f\"Test Labels Shape: {test_labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Select an image and label to display\n",
    "index = 1  # Change this to see other images\n",
    "image = train_images[index].astype(np.float32)  # Convert to float for better rendering\n",
    "label = train_labels[index]\n",
    "\n",
    "# Plot the image\n",
    "plt.imshow(image, cmap='gray', interpolation='nearest')  # Ensure grayscale rendering\n",
    "plt.title(f\"Label: {label}\")\n",
    "plt.axis('off')  # Remove axis for clarity\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        \"\"\"\n",
    "        Custom MNIST dataset.\n",
    "        Args:\n",
    "            images: NumPy array or tensor of shape (N, 28, 28).\n",
    "            labels: NumPy array or tensor of shape (N,).\n",
    "            transform: Optional. A callable that applies transformations to the images.\n",
    "        \"\"\"\n",
    "        # Convert NumPy arrays to tensors during initialization\n",
    "        self.images = torch.tensor(images, dtype=torch.float32) #Default is usually float32\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long) # Type long for classification\n",
    "        self.transform = transform if transform else self.default_transform() #Apply default transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Fetch an image and its corresponding label.\n",
    "        Args:\n",
    "            idx: Index of the image and label to retrieve.\n",
    "        Returns:\n",
    "            Tuple (transformed_image, label).\n",
    "        \"\"\"\n",
    "        # Access the image and label directly as tensors\n",
    "        image = self.images[idx].unsqueeze(0)  # Add channel dimension (C, H, W)\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Apply the transform\n",
    "        image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    def default_transform(self):\n",
    "        \"\"\"\n",
    "        Define the default transformation pipeline for MNIST.\n",
    "        Returns:\n",
    "            A callable transform.\n",
    "        \"\"\"\n",
    "        return transforms.Compose([\n",
    "            transforms.Normalize((0.5,), (0.5,)),  # Normalize to mean 0.5, std 0.5\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 28, 28])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MNISTDataset(train_images, train_labels).images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    Normalize(mean=(0.5,), std=(0.5,))\n",
       ")"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MNISTDataset(train_images, train_labels).default_transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MNISTDataset(train_images, train_labels).__getitem__(1)[0].shape# Return \"image\" at idx 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MNISTDataset(train_images, train_labels).__getitem__(1)[1] # Return label at idx 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(MNISTDataset(train_images, train_labels), batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(MNISTDataset(test_images, test_labels), batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicFCNN(nn.Module):\n",
    "    def __init__(self, input_size=28*28, hidden_sizes=[128, 64], output_size=10):\n",
    "        \"\"\"\n",
    "        Fully connected neural network with dynamic hidden layer sizes.\n",
    "        Args:\n",
    "            input_size (int): Number of input features (e.g., 28*28 for MNIST).\n",
    "            hidden_sizes (list): List of sizes for the hidden layers.\n",
    "            output_size (int): Number of output classes (e.g., 10 for MNIST digits).\n",
    "        \"\"\"\n",
    "        super(DynamicFCNN, self).__init__()\n",
    "        \n",
    "        # Create a list to store layers\n",
    "        layers = []\n",
    "        \n",
    "        # Input layer\n",
    "        layers.append(nn.Linear(input_size, hidden_sizes[0]))\n",
    "        layers.append(nn.ReLU())\n",
    "        \n",
    "        # Hidden layers - It will iterate through so you can enter more layers in the definition \n",
    "        for i in range(len(hidden_sizes) - 1):\n",
    "            layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i + 1]))\n",
    "            layers.append(nn.ReLU())\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(hidden_sizes[-1], output_size))\n",
    "        \n",
    "        # Register all layers in a Sequential module\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor (batch_size, input_size).\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor (batch_size, output_size).\n",
    "        \"\"\"\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SimpleFCNN(nn.Module):\n",
    "    def __init__(self, input_size=28*28, hidden_size=128, output_size=10):\n",
    "        \"\"\"\n",
    "        Fully connected neural network with a single hidden layer.\n",
    "        Args:\n",
    "            input_size (int): Number of input features (e.g., 28*28 for MNIST).\n",
    "            hidden_size (int): Size of the hidden layer.\n",
    "            output_size (int): Number of output classes (e.g., 10 for MNIST digits).\n",
    "        \"\"\"\n",
    "        super(SimpleFCNN, self).__init__()\n",
    "        \n",
    "        # Define the network layers\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),  # Input to hidden layer\n",
    "            nn.ReLU(),                           # Activation function\n",
    "            nn.Linear(hidden_size, output_size)  # Hidden to output layer\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor (batch_size, input_size).\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor (batch_size, output_size).\n",
    "        \"\"\"\n",
    "        x = x.view(x.size(0), -1)  # Flatten the input\n",
    "        return self.network(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DynamicFCNN(\n",
      "  (network): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=64, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "base_model = DynamicFCNN(input_size=28*28, hidden_sizes=[128, 64], output_size=10)\n",
    "print(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the test data.\n",
    "    \n",
    "    Args:\n",
    "        model (torch.nn.Module): The trained model.\n",
    "        test_loader (DataLoader): DataLoader for the test data.\n",
    "    \n",
    "    Returns:\n",
    "        float: Accuracy of the model on the test dataset.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    return 100 * correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    num_epochs=5,\n",
    "    verbose=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a PyTorch model and evaluate its performance.\n",
    "    \n",
    "    Args:\n",
    "        model (torch.nn.Module): The model to train.\n",
    "        train_loader (DataLoader): DataLoader for the training data.\n",
    "        test_loader (DataLoader): DataLoader for the test data.\n",
    "        optimizer (torch.optim.Optimizer): Optimizer for training.\n",
    "        criterion (torch.nn.Module): Loss function.\n",
    "        num_epochs (int): Number of epochs to train.\n",
    "        verbose (bool): If True, prints training progress.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary containing training loss and test accuracy.\n",
    "    \"\"\"\n",
    "    model.train()  # Set model to training mode\n",
    "    train_losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Track loss\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # Average loss for the epoch\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        train_losses.append(epoch_loss)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
    "    \n",
    "    # Evaluate the model on the test set\n",
    "    test_accuracy = evaluate_model(model, test_loader)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.0736\n",
      "Epoch [2/5], Loss: 0.0709\n",
      "Epoch [3/5], Loss: 0.0576\n",
      "Epoch [4/5], Loss: 0.0579\n",
      "Epoch [5/5], Loss: 0.0598\n",
      "Test Accuracy: 96.98%\n"
     ]
    }
   ],
   "source": [
    "train_model(\n",
    "    model=base_model,\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    optimizer=torch.optim.Adam(base_model.parameters(), lr=0.001),\n",
    "    criterion=torch.nn.CrossEntropyLoss(),\n",
    "    num_epochs=5,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
